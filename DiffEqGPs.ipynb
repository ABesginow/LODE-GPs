{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eaef263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "from kernels import *\n",
    "import pdb\n",
    "import gpytorch\n",
    "from itertools import product\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "779684f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.linspace(float(0), float(1), int(50))\n",
    "one = torch.sin(train_x * (float(2) * math.pi)) + torch.randn(train_x.size()) * float(0.2)\n",
    "two = torch.cos(train_x * (float(2) * math.pi)) + torch.randn(train_x.size()) * float(0.2)\n",
    "train_y = torch.stack([one, two], int(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "361022cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8734672a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15ef0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d5dedb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of all kernels: [Diff_SE_kernel(), Diff_SE_kernel()]\n",
      "[[AdditiveKernel(\n",
      "  (kernels): ModuleList(\n",
      "    (0): diffed_SE_kernel()\n",
      "    (1): diffed_SE_kernel()\n",
      "  )\n",
      "), AdditiveKernel(\n",
      "  (kernels): ModuleList(\n",
      "    (0): diffed_SE_kernel()\n",
      "    (1): diffed_SE_kernel()\n",
      "  )\n",
      ")], [AdditiveKernel(\n",
      "  (kernels): ModuleList(\n",
      "    (0): diffed_SE_kernel()\n",
      "    (1): diffed_SE_kernel()\n",
      "  )\n",
      "), AdditiveKernel(\n",
      "  (kernels): ModuleList(\n",
      "    (0): diffed_SE_kernel()\n",
      "    (1): diffed_SE_kernel()\n",
      "  )\n",
      ")]]\n"
     ]
    }
   ],
   "source": [
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ZeroMean(), num_tasks=2\n",
    "        )\n",
    "        kernel = Diff_SE_kernel()\n",
    "        kernel2 = Diff_SE_kernel()\n",
    "        q, dx1, dx2 = var('q, dx1, dx2')\n",
    "        L = matrix(2, 2, (dx1, dx1, 0, dx1))\n",
    "        R = matrix(2, 2, (dx2, 0, dx2, dx2))\n",
    "        p = DiffMatrixKernel([[kernel, None], [None, kernel2]])\n",
    "        self.covar_module = p.diff(left_matrix=L, right_matrix=R)\n",
    "        #kernel0 = Diff_SE_kernel()\n",
    "        #kernel1 = Diff_SE_kernel()\n",
    "        #kernel2 = Diff_SE_kernel()\n",
    "        #self.covar_module = MatrixKernel([[kernel0, None], [None, kernel2]])\n",
    "\n",
    "    def forward(self, x):\n",
    "        #pdb.set_trace()\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        #print(f\"{covar_x.detach().evaluate()}\")\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x, validate_args=True)\n",
    "\n",
    "\n",
    "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=2)\n",
    "model = MultitaskGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f0a9b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('likelihood.raw_task_noises', Parameter containing:\n",
      "tensor([0., 0.], requires_grad=True))\n",
      "('likelihood.raw_noise', Parameter containing:\n",
      "tensor([0.], requires_grad=True))\n",
      "('covar_module.12015766640.var', Parameter containing:\n",
      "tensor(1., requires_grad=True))\n",
      "('covar_module.12015766640.length', Parameter containing:\n",
      "tensor(1., requires_grad=True))\n",
      "('covar_module.12015765968.var', Parameter containing:\n",
      "tensor(1., requires_grad=True))\n",
      "('covar_module.12015765968.length', Parameter containing:\n",
      "tensor(1., requires_grad=True))\n",
      "Result:\n",
      "tensor([[2.0000, 1.9988, 1.9950,  ..., 0.1010, 0.0500, 0.0000],\n",
      "        [1.9988, 2.0000, 1.9988,  ..., 0.1528, 0.1010, 0.0500],\n",
      "        [1.9950, 1.9988, 2.0000,  ..., 0.2054, 0.1528, 0.1010],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 2.0000, 1.9988, 1.9950],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 1.9988, 2.0000, 1.9988],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 1.9950, 1.9988, 2.0000]],\n",
      "       grad_fn=<CatBackward>)\n",
      "Symmetric result:\n",
      "tensor([[2.0000, 1.9988, 1.9950,  ..., 0.1010, 0.0500, 0.0000],\n",
      "        [1.9988, 2.0000, 1.9988,  ..., 0.1528, 0.1010, 0.0500],\n",
      "        [1.9950, 1.9988, 2.0000,  ..., 0.2054, 0.1528, 0.1010],\n",
      "        ...,\n",
      "        [0.1010, 0.1528, 0.2054,  ..., 2.0000, 1.9988, 1.9950],\n",
      "        [0.0500, 0.1010, 0.1528,  ..., 1.9988, 2.0000, 1.9988],\n",
      "        [0.0000, 0.0500, 0.1010,  ..., 1.9950, 1.9988, 2.0000]],\n",
      "       grad_fn=<CopySlices>)\n",
      "Interleaved result:\n",
      "tensor([[2.0000, 2.0000, 1.9988,  ..., 0.0500, 0.0000, 0.0000],\n",
      "        [2.0000, 2.0000, 1.9988,  ..., 0.0500, 0.0000, 0.0000],\n",
      "        [1.9988, 1.9988, 2.0000,  ..., 0.1010, 0.0500, 0.0500],\n",
      "        ...,\n",
      "        [0.0500, 0.0500, 0.1010,  ..., 2.0000, 1.9988, 1.9988],\n",
      "        [0.0000, 0.0000, 0.0500,  ..., 1.9988, 2.0000, 2.0000],\n",
      "        [0.0000, 0.0000, 0.0500,  ..., 1.9988, 2.0000, 2.0000]],\n",
      "       grad_fn=<CatBackward>)\n",
      "tensor([[2.0000, 2.0000, 1.9988,  ..., 0.0500, 0.0000, 0.0000],\n",
      "        [2.0000, 2.0000, 1.9988,  ..., 0.0500, 0.0000, 0.0000],\n",
      "        [1.9988, 1.9988, 2.0000,  ..., 0.1010, 0.0500, 0.0500],\n",
      "        ...,\n",
      "        [0.0500, 0.0500, 0.1010,  ..., 2.0000, 1.9988, 1.9988],\n",
      "        [0.0000, 0.0000, 0.0500,  ..., 1.9988, 2.0000, 2.0000],\n",
      "        [0.0000, 0.0000, 0.0500,  ..., 1.9988, 2.0000, 2.0000]],\n",
      "       grad_fn=<CatBackward>)\n",
      "('likelihood.raw_task_noises', Parameter containing:\n",
      "tensor([-0.1000, -0.1000], requires_grad=True))\n",
      "('likelihood.raw_noise', Parameter containing:\n",
      "tensor([-0.1000], requires_grad=True))\n",
      "('covar_module.12015766640.var', Parameter containing:\n",
      "tensor(0.9000, requires_grad=True))\n",
      "('covar_module.12015766640.length', Parameter containing:\n",
      "tensor(0.9000, requires_grad=True))\n",
      "('covar_module.12015765968.var', Parameter containing:\n",
      "tensor(0.9000, requires_grad=True))\n",
      "('covar_module.12015765968.length', Parameter containing:\n",
      "tensor(0.9000, requires_grad=True))\n",
      "Result:\n",
      "tensor([[ 2.2222,  2.2205,  2.2154,  ..., -0.1711, -0.2270, -0.2812],\n",
      "        [ 2.2205,  2.2222,  2.2205,  ..., -0.1135, -0.1711, -0.2270],\n",
      "        [ 2.2154,  2.2205,  2.2222,  ..., -0.0544, -0.1135, -0.1711],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  2.2222,  2.2205,  2.2154],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  2.2205,  2.2222,  2.2205],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  2.2154,  2.2205,  2.2222]],\n",
      "       grad_fn=<CatBackward>)\n",
      "Symmetric result:\n",
      "tensor([[ 2.2222,  2.2205,  2.2154,  ..., -0.1711, -0.2270, -0.2812],\n",
      "        [ 2.2205,  2.2222,  2.2205,  ..., -0.1135, -0.1711, -0.2270],\n",
      "        [ 2.2154,  2.2205,  2.2222,  ..., -0.0544, -0.1135, -0.1711],\n",
      "        ...,\n",
      "        [-0.1711, -0.1135, -0.0544,  ...,  2.2222,  2.2205,  2.2154],\n",
      "        [-0.2270, -0.1711, -0.1135,  ...,  2.2205,  2.2222,  2.2205],\n",
      "        [-0.2812, -0.2270, -0.1711,  ...,  2.2154,  2.2205,  2.2222]],\n",
      "       grad_fn=<CopySlices>)\n",
      "Interleaved result:\n",
      "tensor([[ 2.2222,  2.2222,  2.2205,  ..., -0.2270, -0.2812, -0.2812],\n",
      "        [ 2.2222,  2.2222,  2.2205,  ..., -0.2270, -0.2812, -0.2812],\n",
      "        [ 2.2205,  2.2205,  2.2222,  ..., -0.1711, -0.2270, -0.2270],\n",
      "        ...,\n",
      "        [-0.2270, -0.2270, -0.1711,  ...,  2.2222,  2.2205,  2.2205],\n",
      "        [-0.2812, -0.2812, -0.2270,  ...,  2.2205,  2.2222,  2.2222],\n",
      "        [-0.2812, -0.2812, -0.2270,  ...,  2.2205,  2.2222,  2.2222]],\n",
      "       grad_fn=<CatBackward>)\n",
      "tensor([[ 2.2222,  2.2222,  2.2205,  ..., -0.2270, -0.2812, -0.2812],\n",
      "        [ 2.2222,  2.2222,  2.2205,  ..., -0.2270, -0.2812, -0.2812],\n",
      "        [ 2.2205,  2.2205,  2.2222,  ..., -0.1711, -0.2270, -0.2270],\n",
      "        ...,\n",
      "        [-0.2270, -0.2270, -0.1711,  ...,  2.2222,  2.2205,  2.2205],\n",
      "        [-0.2812, -0.2812, -0.2270,  ...,  2.2205,  2.2222,  2.2222],\n",
      "        [-0.2812, -0.2812, -0.2270,  ...,  2.2205,  2.2222,  2.2222]],\n",
      "       grad_fn=<CatBackward>)\n",
      "('likelihood.raw_task_noises', Parameter containing:\n",
      "tensor([-0.2000, -0.2001], requires_grad=True))\n",
      "('likelihood.raw_noise', Parameter containing:\n",
      "tensor([-0.2000], requires_grad=True))\n",
      "('covar_module.12015766640.var', Parameter containing:\n",
      "tensor(0.8012, requires_grad=True))\n",
      "('covar_module.12015766640.length', Parameter containing:\n",
      "tensor(0.7999, requires_grad=True))\n",
      "('covar_module.12015765968.var', Parameter containing:\n",
      "tensor(0.8012, requires_grad=True))\n",
      "('covar_module.12015765968.length', Parameter containing:\n",
      "tensor(0.7999, requires_grad=True))\n",
      "Result:\n",
      "tensor([[ 2.5045,  2.5020,  2.4947,  ..., -0.5345, -0.5914, -0.6454],\n",
      "        [ 2.5020,  2.5045,  2.5020,  ..., -0.4748, -0.5345, -0.5914],\n",
      "        [ 2.4947,  2.5020,  2.5045,  ..., -0.4123, -0.4748, -0.5345],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  2.5045,  2.5020,  2.4947],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  2.5020,  2.5045,  2.5020],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  2.4947,  2.5020,  2.5045]],\n",
      "       grad_fn=<CatBackward>)\n",
      "Symmetric result:\n",
      "tensor([[ 2.5045,  2.5020,  2.4947,  ..., -0.5345, -0.5914, -0.6454],\n",
      "        [ 2.5020,  2.5045,  2.5020,  ..., -0.4748, -0.5345, -0.5914],\n",
      "        [ 2.4947,  2.5020,  2.5045,  ..., -0.4123, -0.4748, -0.5345],\n",
      "        ...,\n",
      "        [-0.5345, -0.4748, -0.4123,  ...,  2.5045,  2.5020,  2.4947],\n",
      "        [-0.5914, -0.5345, -0.4748,  ...,  2.5020,  2.5045,  2.5020],\n",
      "        [-0.6454, -0.5914, -0.5345,  ...,  2.4947,  2.5020,  2.5045]],\n",
      "       grad_fn=<CopySlices>)\n",
      "Interleaved result:\n",
      "tensor([[ 2.5045,  2.5045,  2.5020,  ..., -0.5914, -0.6454, -0.6454],\n",
      "        [ 2.5045,  2.5045,  2.5020,  ..., -0.5914, -0.6454, -0.6454],\n",
      "        [ 2.5020,  2.5020,  2.5045,  ..., -0.5345, -0.5914, -0.5914],\n",
      "        ...,\n",
      "        [-0.5914, -0.5914, -0.5345,  ...,  2.5045,  2.5020,  2.5020],\n",
      "        [-0.6454, -0.6454, -0.5914,  ...,  2.5020,  2.5045,  2.5045],\n",
      "        [-0.6454, -0.6454, -0.5914,  ...,  2.5020,  2.5045,  2.5045]],\n",
      "       grad_fn=<CatBackward>)\n",
      "tensor([[ 2.5045,  2.5045,  2.5020,  ..., -0.5914, -0.6454, -0.6454],\n",
      "        [ 2.5045,  2.5045,  2.5020,  ..., -0.5914, -0.6454, -0.6454],\n",
      "        [ 2.5020,  2.5020,  2.5045,  ..., -0.5345, -0.5914, -0.5914],\n",
      "        ...,\n",
      "        [-0.5914, -0.5914, -0.5345,  ...,  2.5045,  2.5020,  2.5020],\n",
      "        [-0.6454, -0.6454, -0.5914,  ...,  2.5020,  2.5045,  2.5045],\n",
      "        [-0.6454, -0.6454, -0.5914,  ...,  2.5020,  2.5045,  2.5045]],\n",
      "       grad_fn=<CatBackward>)\n",
      "('likelihood.raw_task_noises', Parameter containing:\n",
      "tensor([-0.2999, -0.3003], requires_grad=True))\n",
      "('likelihood.raw_noise', Parameter containing:\n",
      "tensor([-0.3001], requires_grad=True))\n",
      "('covar_module.12015766640.var', Parameter containing:\n",
      "tensor(0.7060, requires_grad=True))\n",
      "('covar_module.12015766640.length', Parameter containing:\n",
      "tensor(0.6999, requires_grad=True))\n",
      "('covar_module.12015765968.var', Parameter containing:\n",
      "tensor(0.7060, requires_grad=True))\n",
      "('covar_module.12015765968.length', Parameter containing:\n",
      "tensor(0.6999, requires_grad=True))\n",
      "Result:\n",
      "tensor([[ 2.8828,  2.8791,  2.8681,  ..., -0.9899, -1.0381, -1.0818],\n",
      "        [ 2.8791,  2.8828,  2.8791,  ..., -0.9371, -0.9899, -1.0381],\n",
      "        [ 2.8681,  2.8791,  2.8828,  ..., -0.8797, -0.9371, -0.9899],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  2.8828,  2.8791,  2.8681],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  2.8791,  2.8828,  2.8791],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  2.8681,  2.8791,  2.8828]],\n",
      "       grad_fn=<CatBackward>)\n",
      "Symmetric result:\n",
      "tensor([[ 2.8828,  2.8791,  2.8681,  ..., -0.9899, -1.0381, -1.0818],\n",
      "        [ 2.8791,  2.8828,  2.8791,  ..., -0.9371, -0.9899, -1.0381],\n",
      "        [ 2.8681,  2.8791,  2.8828,  ..., -0.8797, -0.9371, -0.9899],\n",
      "        ...,\n",
      "        [-0.9899, -0.9371, -0.8797,  ...,  2.8828,  2.8791,  2.8681],\n",
      "        [-1.0381, -0.9899, -0.9371,  ...,  2.8791,  2.8828,  2.8791],\n",
      "        [-1.0818, -1.0381, -0.9899,  ...,  2.8681,  2.8791,  2.8828]],\n",
      "       grad_fn=<CopySlices>)\n",
      "Interleaved result:\n",
      "tensor([[ 2.8828,  2.8828,  2.8791,  ..., -1.0381, -1.0818, -1.0818],\n",
      "        [ 2.8828,  2.8828,  2.8791,  ..., -1.0381, -1.0818, -1.0818],\n",
      "        [ 2.8791,  2.8791,  2.8828,  ..., -0.9899, -1.0381, -1.0381],\n",
      "        ...,\n",
      "        [-1.0381, -1.0381, -0.9899,  ...,  2.8828,  2.8791,  2.8791],\n",
      "        [-1.0818, -1.0818, -1.0381,  ...,  2.8791,  2.8828,  2.8828],\n",
      "        [-1.0818, -1.0818, -1.0381,  ...,  2.8791,  2.8828,  2.8828]],\n",
      "       grad_fn=<CatBackward>)\n",
      "tensor([[ 2.8828,  2.8828,  2.8791,  ..., -1.0381, -1.0818, -1.0818],\n",
      "        [ 2.8828,  2.8828,  2.8791,  ..., -1.0381, -1.0818, -1.0818],\n",
      "        [ 2.8791,  2.8791,  2.8828,  ..., -0.9899, -1.0381, -1.0381],\n",
      "        ...,\n",
      "        [-1.0381, -1.0381, -0.9899,  ...,  2.8828,  2.8791,  2.8791],\n",
      "        [-1.0818, -1.0818, -1.0381,  ...,  2.8791,  2.8828,  2.8828],\n",
      "        [-1.0818, -1.0818, -1.0381,  ...,  2.8791,  2.8828,  2.8828]],\n",
      "       grad_fn=<CatBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('likelihood.raw_task_noises', Parameter containing:\n",
      "tensor([-0.3998, -0.4006], requires_grad=True))\n",
      "('likelihood.raw_noise', Parameter containing:\n",
      "tensor([-0.4002], requires_grad=True))\n",
      "('covar_module.12015766640.var', Parameter containing:\n",
      "tensor(0.6141, requires_grad=True))\n",
      "('covar_module.12015766640.length', Parameter containing:\n",
      "tensor(0.6022, requires_grad=True))\n",
      "('covar_module.12015765968.var', Parameter containing:\n",
      "tensor(0.6141, requires_grad=True))\n",
      "('covar_module.12015765968.length', Parameter containing:\n",
      "tensor(0.6022, requires_grad=True))\n",
      "Result:\n",
      "tensor([[ 3.3867,  3.3809,  3.3634,  ..., -1.4640, -1.4847, -1.4993],\n",
      "        [ 3.3809,  3.3867,  3.3809,  ..., -1.4370, -1.4640, -1.4847],\n",
      "        [ 3.3634,  3.3809,  3.3867,  ..., -1.4034, -1.4370, -1.4640],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  3.3867,  3.3809,  3.3634],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  3.3809,  3.3867,  3.3809],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  3.3634,  3.3809,  3.3867]],\n",
      "       grad_fn=<CatBackward>)\n",
      "Symmetric result:\n",
      "tensor([[ 3.3867,  3.3809,  3.3634,  ..., -1.4640, -1.4847, -1.4993],\n",
      "        [ 3.3809,  3.3867,  3.3809,  ..., -1.4370, -1.4640, -1.4847],\n",
      "        [ 3.3634,  3.3809,  3.3867,  ..., -1.4034, -1.4370, -1.4640],\n",
      "        ...,\n",
      "        [-1.4640, -1.4370, -1.4034,  ...,  3.3867,  3.3809,  3.3634],\n",
      "        [-1.4847, -1.4640, -1.4370,  ...,  3.3809,  3.3867,  3.3809],\n",
      "        [-1.4993, -1.4847, -1.4640,  ...,  3.3634,  3.3809,  3.3867]],\n",
      "       grad_fn=<CopySlices>)\n",
      "Interleaved result:\n",
      "tensor([[ 3.3867,  3.3867,  3.3809,  ..., -1.4847, -1.4993, -1.4993],\n",
      "        [ 3.3867,  3.3867,  3.3809,  ..., -1.4847, -1.4993, -1.4993],\n",
      "        [ 3.3809,  3.3809,  3.3867,  ..., -1.4640, -1.4847, -1.4847],\n",
      "        ...,\n",
      "        [-1.4847, -1.4847, -1.4640,  ...,  3.3867,  3.3809,  3.3809],\n",
      "        [-1.4993, -1.4993, -1.4847,  ...,  3.3809,  3.3867,  3.3867],\n",
      "        [-1.4993, -1.4993, -1.4847,  ...,  3.3809,  3.3867,  3.3867]],\n",
      "       grad_fn=<CatBackward>)\n",
      "tensor([[ 3.3867,  3.3867,  3.3809,  ..., -1.4847, -1.4993, -1.4993],\n",
      "        [ 3.3867,  3.3867,  3.3809,  ..., -1.4847, -1.4993, -1.4993],\n",
      "        [ 3.3809,  3.3809,  3.3867,  ..., -1.4640, -1.4847, -1.4847],\n",
      "        ...,\n",
      "        [-1.4847, -1.4847, -1.4640,  ...,  3.3867,  3.3809,  3.3809],\n",
      "        [-1.4993, -1.4993, -1.4847,  ...,  3.3809,  3.3867,  3.3867],\n",
      "        [-1.4993, -1.4993, -1.4847,  ...,  3.3809,  3.3867,  3.3867]],\n",
      "       grad_fn=<CatBackward>)\n",
      "('likelihood.raw_task_noises', Parameter containing:\n",
      "tensor([-0.4998, -0.5010], requires_grad=True))\n",
      "('likelihood.raw_noise', Parameter containing:\n",
      "tensor([-0.5004], requires_grad=True))\n",
      "('covar_module.12015766640.var', Parameter containing:\n",
      "tensor(0.5259, requires_grad=True))\n",
      "('covar_module.12015766640.length', Parameter containing:\n",
      "tensor(0.5387, requires_grad=True))\n",
      "('covar_module.12015765968.var', Parameter containing:\n",
      "tensor(0.5259, requires_grad=True))\n",
      "('covar_module.12015765968.length', Parameter containing:\n",
      "tensor(0.5387, requires_grad=True))\n",
      "Result:\n",
      "tensor([[ 3.6253,  3.6175,  3.5942,  ..., -1.6123, -1.6006, -1.5831],\n",
      "        [ 3.6175,  3.6253,  3.6175,  ..., -1.6176, -1.6123, -1.6006],\n",
      "        [ 3.5942,  3.6175,  3.6253,  ..., -1.6160, -1.6176, -1.6123],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  3.6253,  3.6175,  3.5942],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  3.6175,  3.6253,  3.6175],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  3.5942,  3.6175,  3.6253]],\n",
      "       grad_fn=<CatBackward>)\n",
      "Symmetric result:\n",
      "tensor([[ 3.6253,  3.6175,  3.5942,  ..., -1.6123, -1.6006, -1.5831],\n",
      "        [ 3.6175,  3.6253,  3.6175,  ..., -1.6176, -1.6123, -1.6006],\n",
      "        [ 3.5942,  3.6175,  3.6253,  ..., -1.6160, -1.6176, -1.6123],\n",
      "        ...,\n",
      "        [-1.6123, -1.6176, -1.6160,  ...,  3.6253,  3.6175,  3.5942],\n",
      "        [-1.6006, -1.6123, -1.6176,  ...,  3.6175,  3.6253,  3.6175],\n",
      "        [-1.5831, -1.6006, -1.6123,  ...,  3.5942,  3.6175,  3.6253]],\n",
      "       grad_fn=<CopySlices>)\n",
      "Interleaved result:\n",
      "tensor([[ 3.6253,  3.6253,  3.6175,  ..., -1.6006, -1.5831, -1.5831],\n",
      "        [ 3.6253,  3.6253,  3.6175,  ..., -1.6006, -1.5831, -1.5831],\n",
      "        [ 3.6175,  3.6175,  3.6253,  ..., -1.6123, -1.6006, -1.6006],\n",
      "        ...,\n",
      "        [-1.6006, -1.6006, -1.6123,  ...,  3.6253,  3.6175,  3.6175],\n",
      "        [-1.5831, -1.5831, -1.6006,  ...,  3.6175,  3.6253,  3.6253],\n",
      "        [-1.5831, -1.5831, -1.6006,  ...,  3.6175,  3.6253,  3.6253]],\n",
      "       grad_fn=<CatBackward>)\n",
      "tensor([[ 3.6253,  3.6253,  3.6175,  ..., -1.6006, -1.5831, -1.5831],\n",
      "        [ 3.6253,  3.6253,  3.6175,  ..., -1.6006, -1.5831, -1.5831],\n",
      "        [ 3.6175,  3.6175,  3.6253,  ..., -1.6123, -1.6006, -1.6006],\n",
      "        ...,\n",
      "        [-1.6006, -1.6006, -1.6123,  ...,  3.6253,  3.6175,  3.6175],\n",
      "        [-1.5831, -1.5831, -1.6006,  ...,  3.6175,  3.6253,  3.6253],\n",
      "        [-1.5831, -1.5831, -1.6006,  ...,  3.6175,  3.6253,  3.6253]],\n",
      "       grad_fn=<CatBackward>)\n",
      "('likelihood.raw_task_noises', Parameter containing:\n",
      "tensor([-0.5998, -0.6013], requires_grad=True))\n",
      "('likelihood.raw_noise', Parameter containing:\n",
      "tensor([-0.6006], requires_grad=True))\n",
      "('covar_module.12015766640.var', Parameter containing:\n",
      "tensor(0.4383, requires_grad=True))\n",
      "('covar_module.12015766640.length', Parameter containing:\n",
      "tensor(0.5410, requires_grad=True))\n",
      "('covar_module.12015765968.var', Parameter containing:\n",
      "tensor(0.4383, requires_grad=True))\n",
      "('covar_module.12015765968.length', Parameter containing:\n",
      "tensor(0.5410, requires_grad=True))\n",
      "Result:\n",
      "tensor([[ 2.9948,  2.9884,  2.9692,  ..., -1.3332, -1.3247, -1.3112],\n",
      "        [ 2.9884,  2.9948,  2.9884,  ..., -1.3364, -1.3332, -1.3247],\n",
      "        [ 2.9692,  2.9884,  2.9948,  ..., -1.3340, -1.3364, -1.3332],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  2.9948,  2.9884,  2.9692],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  2.9884,  2.9948,  2.9884],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  2.9692,  2.9884,  2.9948]],\n",
      "       grad_fn=<CatBackward>)\n",
      "Symmetric result:\n",
      "tensor([[ 2.9948,  2.9884,  2.9692,  ..., -1.3332, -1.3247, -1.3112],\n",
      "        [ 2.9884,  2.9948,  2.9884,  ..., -1.3364, -1.3332, -1.3247],\n",
      "        [ 2.9692,  2.9884,  2.9948,  ..., -1.3340, -1.3364, -1.3332],\n",
      "        ...,\n",
      "        [-1.3332, -1.3364, -1.3340,  ...,  2.9948,  2.9884,  2.9692],\n",
      "        [-1.3247, -1.3332, -1.3364,  ...,  2.9884,  2.9948,  2.9884],\n",
      "        [-1.3112, -1.3247, -1.3332,  ...,  2.9692,  2.9884,  2.9948]],\n",
      "       grad_fn=<CopySlices>)\n",
      "Interleaved result:\n",
      "tensor([[ 2.9948,  2.9948,  2.9884,  ..., -1.3247, -1.3112, -1.3112],\n",
      "        [ 2.9948,  2.9948,  2.9884,  ..., -1.3247, -1.3112, -1.3112],\n",
      "        [ 2.9884,  2.9884,  2.9948,  ..., -1.3332, -1.3247, -1.3247],\n",
      "        ...,\n",
      "        [-1.3247, -1.3247, -1.3332,  ...,  2.9948,  2.9884,  2.9884],\n",
      "        [-1.3112, -1.3112, -1.3247,  ...,  2.9884,  2.9948,  2.9948],\n",
      "        [-1.3112, -1.3112, -1.3247,  ...,  2.9884,  2.9948,  2.9948]],\n",
      "       grad_fn=<CatBackward>)\n",
      "tensor([[ 2.9948,  2.9948,  2.9884,  ..., -1.3247, -1.3112, -1.3112],\n",
      "        [ 2.9948,  2.9948,  2.9884,  ..., -1.3247, -1.3112, -1.3112],\n",
      "        [ 2.9884,  2.9884,  2.9948,  ..., -1.3332, -1.3247, -1.3247],\n",
      "        ...,\n",
      "        [-1.3247, -1.3247, -1.3332,  ...,  2.9948,  2.9884,  2.9884],\n",
      "        [-1.3112, -1.3112, -1.3247,  ...,  2.9884,  2.9948,  2.9948],\n",
      "        [-1.3112, -1.3112, -1.3247,  ...,  2.9884,  2.9948,  2.9948]],\n",
      "       grad_fn=<CatBackward>)\n",
      "('likelihood.raw_task_noises', Parameter containing:\n",
      "tensor([-0.6998, -0.7016], requires_grad=True))\n",
      "('likelihood.raw_noise', Parameter containing:\n",
      "tensor([-0.7008], requires_grad=True))\n",
      "('covar_module.12015766640.var', Parameter containing:\n",
      "tensor(0.3473, requires_grad=True))\n",
      "('covar_module.12015766640.length', Parameter containing:\n",
      "tensor(0.5661, requires_grad=True))\n",
      "('covar_module.12015765968.var', Parameter containing:\n",
      "tensor(0.3473, requires_grad=True))\n",
      "('covar_module.12015765968.length', Parameter containing:\n",
      "tensor(0.5661, requires_grad=True))\n",
      "Result:\n",
      "tensor([[ 2.1676,  2.1634,  2.1508,  ..., -0.9652, -0.9673, -0.9656],\n",
      "        [ 2.1634,  2.1676,  2.1634,  ..., -0.9591, -0.9652, -0.9673],\n",
      "        [ 2.1508,  2.1634,  2.1676,  ..., -0.9488, -0.9591, -0.9652],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  2.1676,  2.1634,  2.1508],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  2.1634,  2.1676,  2.1634],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  2.1508,  2.1634,  2.1676]],\n",
      "       grad_fn=<CatBackward>)\n",
      "Symmetric result:\n",
      "tensor([[ 2.1676,  2.1634,  2.1508,  ..., -0.9652, -0.9673, -0.9656],\n",
      "        [ 2.1634,  2.1676,  2.1634,  ..., -0.9591, -0.9652, -0.9673],\n",
      "        [ 2.1508,  2.1634,  2.1676,  ..., -0.9488, -0.9591, -0.9652],\n",
      "        ...,\n",
      "        [-0.9652, -0.9591, -0.9488,  ...,  2.1676,  2.1634,  2.1508],\n",
      "        [-0.9673, -0.9652, -0.9591,  ...,  2.1634,  2.1676,  2.1634],\n",
      "        [-0.9656, -0.9673, -0.9652,  ...,  2.1508,  2.1634,  2.1676]],\n",
      "       grad_fn=<CopySlices>)\n",
      "Interleaved result:\n",
      "tensor([[ 2.1676,  2.1676,  2.1634,  ..., -0.9673, -0.9656, -0.9656],\n",
      "        [ 2.1676,  2.1676,  2.1634,  ..., -0.9673, -0.9656, -0.9656],\n",
      "        [ 2.1634,  2.1634,  2.1676,  ..., -0.9652, -0.9673, -0.9673],\n",
      "        ...,\n",
      "        [-0.9673, -0.9673, -0.9652,  ...,  2.1676,  2.1634,  2.1634],\n",
      "        [-0.9656, -0.9656, -0.9673,  ...,  2.1634,  2.1676,  2.1676],\n",
      "        [-0.9656, -0.9656, -0.9673,  ...,  2.1634,  2.1676,  2.1676]],\n",
      "       grad_fn=<CatBackward>)\n",
      "tensor([[ 2.1676,  2.1676,  2.1634,  ..., -0.9673, -0.9656, -0.9656],\n",
      "        [ 2.1676,  2.1676,  2.1634,  ..., -0.9673, -0.9656, -0.9656],\n",
      "        [ 2.1634,  2.1634,  2.1676,  ..., -0.9652, -0.9673, -0.9673],\n",
      "        ...,\n",
      "        [-0.9673, -0.9673, -0.9652,  ...,  2.1676,  2.1634,  2.1634],\n",
      "        [-0.9656, -0.9656, -0.9673,  ...,  2.1634,  2.1676,  2.1676],\n",
      "        [-0.9656, -0.9656, -0.9673,  ...,  2.1634,  2.1676,  2.1676]],\n",
      "       grad_fn=<CatBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('likelihood.raw_task_noises', Parameter containing:\n",
      "tensor([-0.7995, -0.8017], requires_grad=True))\n",
      "('likelihood.raw_noise', Parameter containing:\n",
      "tensor([-0.8007], requires_grad=True))\n",
      "('covar_module.12015766640.var', Parameter containing:\n",
      "tensor(0.2530, requires_grad=True))\n",
      "('covar_module.12015766640.length', Parameter containing:\n",
      "tensor(0.5838, requires_grad=True))\n",
      "('covar_module.12015765968.var', Parameter containing:\n",
      "tensor(0.2530, requires_grad=True))\n",
      "('covar_module.12015765968.length', Parameter containing:\n",
      "tensor(0.5838, requires_grad=True))\n",
      "Result:\n",
      "tensor([[ 1.4849,  1.4822,  1.4740,  ..., -0.6544, -0.6597, -0.6623],\n",
      "        [ 1.4822,  1.4849,  1.4822,  ..., -0.6463, -0.6544, -0.6597],\n",
      "        [ 1.4740,  1.4822,  1.4849,  ..., -0.6354, -0.6463, -0.6544],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  1.4849,  1.4822,  1.4740],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  1.4822,  1.4849,  1.4822],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  1.4740,  1.4822,  1.4849]],\n",
      "       grad_fn=<CatBackward>)\n",
      "Symmetric result:\n",
      "tensor([[ 1.4849,  1.4822,  1.4740,  ..., -0.6544, -0.6597, -0.6623],\n",
      "        [ 1.4822,  1.4849,  1.4822,  ..., -0.6463, -0.6544, -0.6597],\n",
      "        [ 1.4740,  1.4822,  1.4849,  ..., -0.6354, -0.6463, -0.6544],\n",
      "        ...,\n",
      "        [-0.6544, -0.6463, -0.6354,  ...,  1.4849,  1.4822,  1.4740],\n",
      "        [-0.6597, -0.6544, -0.6463,  ...,  1.4822,  1.4849,  1.4822],\n",
      "        [-0.6623, -0.6597, -0.6544,  ...,  1.4740,  1.4822,  1.4849]],\n",
      "       grad_fn=<CopySlices>)\n",
      "Interleaved result:\n",
      "tensor([[ 1.4849,  1.4849,  1.4822,  ..., -0.6597, -0.6623, -0.6623],\n",
      "        [ 1.4849,  1.4849,  1.4822,  ..., -0.6597, -0.6623, -0.6623],\n",
      "        [ 1.4822,  1.4822,  1.4849,  ..., -0.6544, -0.6597, -0.6597],\n",
      "        ...,\n",
      "        [-0.6597, -0.6597, -0.6544,  ...,  1.4849,  1.4822,  1.4822],\n",
      "        [-0.6623, -0.6623, -0.6597,  ...,  1.4822,  1.4849,  1.4849],\n",
      "        [-0.6623, -0.6623, -0.6597,  ...,  1.4822,  1.4849,  1.4849]],\n",
      "       grad_fn=<CatBackward>)\n",
      "tensor([[ 1.4849,  1.4849,  1.4822,  ..., -0.6597, -0.6623, -0.6623],\n",
      "        [ 1.4849,  1.4849,  1.4822,  ..., -0.6597, -0.6623, -0.6623],\n",
      "        [ 1.4822,  1.4822,  1.4849,  ..., -0.6544, -0.6597, -0.6597],\n",
      "        ...,\n",
      "        [-0.6597, -0.6597, -0.6544,  ...,  1.4849,  1.4822,  1.4822],\n",
      "        [-0.6623, -0.6623, -0.6597,  ...,  1.4822,  1.4849,  1.4849],\n",
      "        [-0.6623, -0.6623, -0.6597,  ...,  1.4822,  1.4849,  1.4849]],\n",
      "       grad_fn=<CatBackward>)\n",
      "('likelihood.raw_task_noises', Parameter containing:\n",
      "tensor([-0.8988, -0.9015], requires_grad=True))\n",
      "('likelihood.raw_noise', Parameter containing:\n",
      "tensor([-0.9003], requires_grad=True))\n",
      "('covar_module.12015766640.var', Parameter containing:\n",
      "tensor(0.1580, requires_grad=True))\n",
      "('covar_module.12015766640.length', Parameter containing:\n",
      "tensor(0.5713, requires_grad=True))\n",
      "('covar_module.12015765968.var', Parameter containing:\n",
      "tensor(0.1580, requires_grad=True))\n",
      "('covar_module.12015765968.length', Parameter containing:\n",
      "tensor(0.5713, requires_grad=True))\n",
      "Result:\n",
      "tensor([[ 0.9685,  0.9666,  0.9611,  ..., -0.4303, -0.4320, -0.4320],\n",
      "        [ 0.9666,  0.9685,  0.9666,  ..., -0.4268, -0.4303, -0.4320],\n",
      "        [ 0.9611,  0.9666,  0.9685,  ..., -0.4215, -0.4268, -0.4303],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.9685,  0.9666,  0.9611],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.9666,  0.9685,  0.9666],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.9611,  0.9666,  0.9685]],\n",
      "       grad_fn=<CatBackward>)\n",
      "Symmetric result:\n",
      "tensor([[ 0.9685,  0.9666,  0.9611,  ..., -0.4303, -0.4320, -0.4320],\n",
      "        [ 0.9666,  0.9685,  0.9666,  ..., -0.4268, -0.4303, -0.4320],\n",
      "        [ 0.9611,  0.9666,  0.9685,  ..., -0.4215, -0.4268, -0.4303],\n",
      "        ...,\n",
      "        [-0.4303, -0.4268, -0.4215,  ...,  0.9685,  0.9666,  0.9611],\n",
      "        [-0.4320, -0.4303, -0.4268,  ...,  0.9666,  0.9685,  0.9666],\n",
      "        [-0.4320, -0.4320, -0.4303,  ...,  0.9611,  0.9666,  0.9685]],\n",
      "       grad_fn=<CopySlices>)\n",
      "Interleaved result:\n",
      "tensor([[ 0.9685,  0.9685,  0.9666,  ..., -0.4320, -0.4320, -0.4320],\n",
      "        [ 0.9685,  0.9685,  0.9666,  ..., -0.4320, -0.4320, -0.4320],\n",
      "        [ 0.9666,  0.9666,  0.9685,  ..., -0.4303, -0.4320, -0.4320],\n",
      "        ...,\n",
      "        [-0.4320, -0.4320, -0.4303,  ...,  0.9685,  0.9666,  0.9666],\n",
      "        [-0.4320, -0.4320, -0.4320,  ...,  0.9666,  0.9685,  0.9685],\n",
      "        [-0.4320, -0.4320, -0.4320,  ...,  0.9666,  0.9685,  0.9685]],\n",
      "       grad_fn=<CatBackward>)\n",
      "tensor([[ 0.9685,  0.9685,  0.9666,  ..., -0.4320, -0.4320, -0.4320],\n",
      "        [ 0.9685,  0.9685,  0.9666,  ..., -0.4320, -0.4320, -0.4320],\n",
      "        [ 0.9666,  0.9666,  0.9685,  ..., -0.4303, -0.4320, -0.4320],\n",
      "        ...,\n",
      "        [-0.4320, -0.4320, -0.4303,  ...,  0.9685,  0.9666,  0.9666],\n",
      "        [-0.4320, -0.4320, -0.4320,  ...,  0.9666,  0.9685,  0.9685],\n",
      "        [-0.4320, -0.4320, -0.4320,  ...,  0.9666,  0.9685,  0.9685]],\n",
      "       grad_fn=<CatBackward>)\n",
      "('likelihood.raw_task_noises', Parameter containing:\n",
      "tensor([-0.9976, -1.0008], requires_grad=True))\n",
      "('likelihood.raw_noise', Parameter containing:\n",
      "tensor([-0.9993], requires_grad=True))\n",
      "('covar_module.12015766640.var', Parameter containing:\n",
      "tensor(0.0689, requires_grad=True))\n",
      "('covar_module.12015766640.length', Parameter containing:\n",
      "tensor(0.5306, requires_grad=True))\n",
      "('covar_module.12015765968.var', Parameter containing:\n",
      "tensor(0.0689, requires_grad=True))\n",
      "('covar_module.12015765968.length', Parameter containing:\n",
      "tensor(0.5306, requires_grad=True))\n",
      "Result:\n",
      "tensor([[ 0.4893,  0.4882,  0.4849,  ..., -0.2166, -0.2144, -0.2114],\n",
      "        [ 0.4882,  0.4893,  0.4882,  ..., -0.2179, -0.2166, -0.2144],\n",
      "        [ 0.4849,  0.4882,  0.4893,  ..., -0.2183, -0.2179, -0.2166],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.4893,  0.4882,  0.4849],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.4882,  0.4893,  0.4882],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.4849,  0.4882,  0.4893]],\n",
      "       grad_fn=<CatBackward>)\n",
      "Symmetric result:\n",
      "tensor([[ 0.4893,  0.4882,  0.4849,  ..., -0.2166, -0.2144, -0.2114],\n",
      "        [ 0.4882,  0.4893,  0.4882,  ..., -0.2179, -0.2166, -0.2144],\n",
      "        [ 0.4849,  0.4882,  0.4893,  ..., -0.2183, -0.2179, -0.2166],\n",
      "        ...,\n",
      "        [-0.2166, -0.2179, -0.2183,  ...,  0.4893,  0.4882,  0.4849],\n",
      "        [-0.2144, -0.2166, -0.2179,  ...,  0.4882,  0.4893,  0.4882],\n",
      "        [-0.2114, -0.2144, -0.2166,  ...,  0.4849,  0.4882,  0.4893]],\n",
      "       grad_fn=<CopySlices>)\n",
      "Interleaved result:\n",
      "tensor([[ 0.4893,  0.4893,  0.4882,  ..., -0.2144, -0.2114, -0.2114],\n",
      "        [ 0.4893,  0.4893,  0.4882,  ..., -0.2144, -0.2114, -0.2114],\n",
      "        [ 0.4882,  0.4882,  0.4893,  ..., -0.2166, -0.2144, -0.2144],\n",
      "        ...,\n",
      "        [-0.2144, -0.2144, -0.2166,  ...,  0.4893,  0.4882,  0.4882],\n",
      "        [-0.2114, -0.2114, -0.2144,  ...,  0.4882,  0.4893,  0.4893],\n",
      "        [-0.2114, -0.2114, -0.2144,  ...,  0.4882,  0.4893,  0.4893]],\n",
      "       grad_fn=<CatBackward>)\n",
      "tensor([[ 0.4893,  0.4893,  0.4882,  ..., -0.2144, -0.2114, -0.2114],\n",
      "        [ 0.4893,  0.4893,  0.4882,  ..., -0.2144, -0.2114, -0.2114],\n",
      "        [ 0.4882,  0.4882,  0.4893,  ..., -0.2166, -0.2144, -0.2144],\n",
      "        ...,\n",
      "        [-0.2144, -0.2144, -0.2166,  ...,  0.4893,  0.4882,  0.4882],\n",
      "        [-0.2114, -0.2114, -0.2144,  ...,  0.4882,  0.4893,  0.4893],\n",
      "        [-0.2114, -0.2114, -0.2144,  ...,  0.4882,  0.4893,  0.4893]],\n",
      "       grad_fn=<CatBackward>)\n",
      "('likelihood.raw_task_noises', Parameter containing:\n",
      "tensor([-1.0958, -1.0993], requires_grad=True))\n",
      "('likelihood.raw_noise', Parameter containing:\n",
      "tensor([-1.0977], requires_grad=True))\n",
      "('covar_module.12015766640.var', Parameter containing:\n",
      "tensor(0.0529, requires_grad=True))\n",
      "('covar_module.12015766640.length', Parameter containing:\n",
      "tensor(0.4705, requires_grad=True))\n",
      "('covar_module.12015765968.var', Parameter containing:\n",
      "tensor(0.0529, requires_grad=True))\n",
      "('covar_module.12015765968.length', Parameter containing:\n",
      "tensor(0.4705, requires_grad=True))\n",
      "Result:\n",
      "tensor([[ 0.4781,  0.4768,  0.4727,  ..., -0.1889, -0.1825, -0.1757],\n",
      "        [ 0.4768,  0.4781,  0.4768,  ..., -0.1947, -0.1889, -0.1825],\n",
      "        [ 0.4727,  0.4768,  0.4781,  ..., -0.1999, -0.1947, -0.1889],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.4781,  0.4768,  0.4727],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.4768,  0.4781,  0.4768],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.4727,  0.4768,  0.4781]],\n",
      "       grad_fn=<CatBackward>)\n",
      "Symmetric result:\n",
      "tensor([[ 0.4781,  0.4768,  0.4727,  ..., -0.1889, -0.1825, -0.1757],\n",
      "        [ 0.4768,  0.4781,  0.4768,  ..., -0.1947, -0.1889, -0.1825],\n",
      "        [ 0.4727,  0.4768,  0.4781,  ..., -0.1999, -0.1947, -0.1889],\n",
      "        ...,\n",
      "        [-0.1889, -0.1947, -0.1999,  ...,  0.4781,  0.4768,  0.4727],\n",
      "        [-0.1825, -0.1889, -0.1947,  ...,  0.4768,  0.4781,  0.4768],\n",
      "        [-0.1757, -0.1825, -0.1889,  ...,  0.4727,  0.4768,  0.4781]],\n",
      "       grad_fn=<CopySlices>)\n",
      "Interleaved result:\n",
      "tensor([[ 0.4781,  0.4781,  0.4768,  ..., -0.1825, -0.1757, -0.1757],\n",
      "        [ 0.4781,  0.4781,  0.4768,  ..., -0.1825, -0.1757, -0.1757],\n",
      "        [ 0.4768,  0.4768,  0.4781,  ..., -0.1889, -0.1825, -0.1825],\n",
      "        ...,\n",
      "        [-0.1825, -0.1825, -0.1889,  ...,  0.4781,  0.4768,  0.4768],\n",
      "        [-0.1757, -0.1757, -0.1825,  ...,  0.4768,  0.4781,  0.4781],\n",
      "        [-0.1757, -0.1757, -0.1825,  ...,  0.4768,  0.4781,  0.4781]],\n",
      "       grad_fn=<CatBackward>)\n",
      "tensor([[ 0.4781,  0.4781,  0.4768,  ..., -0.1825, -0.1757, -0.1757],\n",
      "        [ 0.4781,  0.4781,  0.4768,  ..., -0.1825, -0.1757, -0.1757],\n",
      "        [ 0.4768,  0.4768,  0.4781,  ..., -0.1889, -0.1825, -0.1825],\n",
      "        ...,\n",
      "        [-0.1825, -0.1825, -0.1889,  ...,  0.4781,  0.4768,  0.4768],\n",
      "        [-0.1757, -0.1757, -0.1825,  ...,  0.4768,  0.4781,  0.4781],\n",
      "        [-0.1757, -0.1757, -0.1825,  ...,  0.4768,  0.4781,  0.4781]],\n",
      "       grad_fn=<CatBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('likelihood.raw_task_noises', Parameter containing:\n",
      "tensor([-1.1932, -1.1970], requires_grad=True))\n",
      "('likelihood.raw_noise', Parameter containing:\n",
      "tensor([-1.1952], requires_grad=True))\n",
      "('covar_module.12015766640.var', Parameter containing:\n",
      "tensor(0.0439, requires_grad=True))\n",
      "('covar_module.12015766640.length', Parameter containing:\n",
      "tensor(0.3997, requires_grad=True))\n",
      "('covar_module.12015765968.var', Parameter containing:\n",
      "tensor(0.0439, requires_grad=True))\n",
      "('covar_module.12015765968.length', Parameter containing:\n",
      "tensor(0.3997, requires_grad=True))\n",
      "Result:\n",
      "tensor([[ 0.5500,  0.5479,  0.5414,  ..., -0.1470, -0.1366, -0.1264],\n",
      "        [ 0.5479,  0.5500,  0.5479,  ..., -0.1574, -0.1470, -0.1366],\n",
      "        [ 0.5414,  0.5479,  0.5500,  ..., -0.1680, -0.1574, -0.1470],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.5500,  0.5479,  0.5414],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.5479,  0.5500,  0.5479],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.5414,  0.5479,  0.5500]],\n",
      "       grad_fn=<CatBackward>)\n",
      "Symmetric result:\n",
      "tensor([[ 0.5500,  0.5479,  0.5414,  ..., -0.1470, -0.1366, -0.1264],\n",
      "        [ 0.5479,  0.5500,  0.5479,  ..., -0.1574, -0.1470, -0.1366],\n",
      "        [ 0.5414,  0.5479,  0.5500,  ..., -0.1680, -0.1574, -0.1470],\n",
      "        ...,\n",
      "        [-0.1470, -0.1574, -0.1680,  ...,  0.5500,  0.5479,  0.5414],\n",
      "        [-0.1366, -0.1470, -0.1574,  ...,  0.5479,  0.5500,  0.5479],\n",
      "        [-0.1264, -0.1366, -0.1470,  ...,  0.5414,  0.5479,  0.5500]],\n",
      "       grad_fn=<CopySlices>)\n",
      "Interleaved result:\n",
      "tensor([[ 0.5500,  0.5500,  0.5479,  ..., -0.1366, -0.1264, -0.1264],\n",
      "        [ 0.5500,  0.5500,  0.5479,  ..., -0.1366, -0.1264, -0.1264],\n",
      "        [ 0.5479,  0.5479,  0.5500,  ..., -0.1470, -0.1366, -0.1366],\n",
      "        ...,\n",
      "        [-0.1366, -0.1366, -0.1470,  ...,  0.5500,  0.5479,  0.5479],\n",
      "        [-0.1264, -0.1264, -0.1366,  ...,  0.5479,  0.5500,  0.5500],\n",
      "        [-0.1264, -0.1264, -0.1366,  ...,  0.5479,  0.5500,  0.5500]],\n",
      "       grad_fn=<CatBackward>)\n",
      "tensor([[ 0.5500,  0.5500,  0.5479,  ..., -0.1366, -0.1264, -0.1264],\n",
      "        [ 0.5500,  0.5500,  0.5479,  ..., -0.1366, -0.1264, -0.1264],\n",
      "        [ 0.5479,  0.5479,  0.5500,  ..., -0.1470, -0.1366, -0.1366],\n",
      "        ...,\n",
      "        [-0.1366, -0.1366, -0.1470,  ...,  0.5500,  0.5479,  0.5479],\n",
      "        [-0.1264, -0.1264, -0.1366,  ...,  0.5479,  0.5500,  0.5500],\n",
      "        [-0.1264, -0.1264, -0.1366,  ...,  0.5479,  0.5500,  0.5500]],\n",
      "       grad_fn=<CatBackward>)\n",
      "('likelihood.raw_task_noises', Parameter containing:\n",
      "tensor([-1.2897, -1.2936], requires_grad=True))\n",
      "('likelihood.raw_noise', Parameter containing:\n",
      "tensor([-1.2918], requires_grad=True))\n",
      "('covar_module.12015766640.var', Parameter containing:\n",
      "tensor(-0.0036, requires_grad=True))\n",
      "('covar_module.12015766640.length', Parameter containing:\n",
      "tensor(0.3381, requires_grad=True))\n",
      "('covar_module.12015765968.var', Parameter containing:\n",
      "tensor(-0.0036, requires_grad=True))\n",
      "('covar_module.12015765968.length', Parameter containing:\n",
      "tensor(0.3381, requires_grad=True))\n",
      "Result:\n",
      "tensor([[-0.0621, -0.0618, -0.0608,  ...,  0.0078,  0.0069,  0.0061],\n",
      "        [-0.0618, -0.0621, -0.0618,  ...,  0.0088,  0.0078,  0.0069],\n",
      "        [-0.0608, -0.0618, -0.0621,  ...,  0.0099,  0.0088,  0.0078],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0621, -0.0618, -0.0608],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0618, -0.0621, -0.0618],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0608, -0.0618, -0.0621]],\n",
      "       grad_fn=<CatBackward>)\n",
      "Symmetric result:\n",
      "tensor([[-0.0621, -0.0618, -0.0608,  ...,  0.0078,  0.0069,  0.0061],\n",
      "        [-0.0618, -0.0621, -0.0618,  ...,  0.0088,  0.0078,  0.0069],\n",
      "        [-0.0608, -0.0618, -0.0621,  ...,  0.0099,  0.0088,  0.0078],\n",
      "        ...,\n",
      "        [ 0.0078,  0.0088,  0.0099,  ..., -0.0621, -0.0618, -0.0608],\n",
      "        [ 0.0069,  0.0078,  0.0088,  ..., -0.0618, -0.0621, -0.0618],\n",
      "        [ 0.0061,  0.0069,  0.0078,  ..., -0.0608, -0.0618, -0.0621]],\n",
      "       grad_fn=<CopySlices>)\n",
      "Interleaved result:\n",
      "tensor([[-0.0621, -0.0621, -0.0618,  ...,  0.0069,  0.0061,  0.0061],\n",
      "        [-0.0621, -0.0621, -0.0618,  ...,  0.0069,  0.0061,  0.0061],\n",
      "        [-0.0618, -0.0618, -0.0621,  ...,  0.0078,  0.0069,  0.0069],\n",
      "        ...,\n",
      "        [ 0.0069,  0.0069,  0.0078,  ..., -0.0621, -0.0618, -0.0618],\n",
      "        [ 0.0061,  0.0061,  0.0069,  ..., -0.0618, -0.0621, -0.0621],\n",
      "        [ 0.0061,  0.0061,  0.0069,  ..., -0.0618, -0.0621, -0.0621]],\n",
      "       grad_fn=<CatBackward>)\n",
      "tensor([[-2.6861e+00,  0.0000e+00],\n",
      "        [-2.1176e+00,  0.0000e+00],\n",
      "        [-9.5719e-01,  0.0000e+00],\n",
      "        [-3.7715e-01,  0.0000e+00],\n",
      "        [-6.2319e-02,  0.0000e+00],\n",
      "        [-9.6412e-03,  0.0000e+00],\n",
      "        [-9.6678e-04,  0.0000e+00],\n",
      "        [-9.7417e-05,  0.0000e+00],\n",
      "        [-7.0707e-06,  0.0000e+00],\n",
      "        [-5.2111e-07,  0.0000e+00],\n",
      "        [-1.5872e-07,  0.0000e+00],\n",
      "        [ 1.2597e-07,  0.0000e+00],\n",
      "        [ 1.1793e-07,  9.0183e-09],\n",
      "        [ 1.1793e-07, -9.0183e-09],\n",
      "        [-1.0791e-07,  0.0000e+00],\n",
      "        [-7.9895e-08,  4.7590e-08],\n",
      "        [-7.9895e-08, -4.7590e-08],\n",
      "        [ 5.8280e-08,  6.2376e-08],\n",
      "        [ 5.8280e-08, -6.2376e-08],\n",
      "        [-8.0334e-08,  0.0000e+00],\n",
      "        [-1.8573e-08,  8.1082e-08],\n",
      "        [-1.8573e-08, -8.1082e-08],\n",
      "        [ 7.7890e-08,  2.5782e-08],\n",
      "        [ 7.7890e-08, -2.5782e-08],\n",
      "        [-5.0276e-08,  6.6351e-08],\n",
      "        [-5.0276e-08, -6.6351e-08],\n",
      "        [-7.7131e-08,  2.7969e-08],\n",
      "        [-7.7131e-08, -2.7969e-08],\n",
      "        [ 5.7878e-08,  2.3062e-08],\n",
      "        [ 5.7878e-08, -2.3062e-08],\n",
      "        [ 5.1177e-08,  0.0000e+00],\n",
      "        [ 2.5136e-08,  4.3171e-08],\n",
      "        [ 2.5136e-08, -4.3171e-08],\n",
      "        [-5.3669e-08,  8.7431e-09],\n",
      "        [-5.3669e-08, -8.7431e-09],\n",
      "        [ 4.8616e-08,  6.5299e-09],\n",
      "        [ 4.8616e-08, -6.5299e-09],\n",
      "        [-3.1824e-08,  3.3159e-08],\n",
      "        [-3.1824e-08, -3.3159e-08],\n",
      "        [-3.7021e-08,  2.5727e-08],\n",
      "        [-3.7021e-08, -2.5727e-08],\n",
      "        [ 2.3031e-08,  3.0572e-08],\n",
      "        [ 2.3031e-08, -3.0572e-08],\n",
      "        [-9.1863e-09,  3.7335e-08],\n",
      "        [-9.1863e-09, -3.7335e-08],\n",
      "        [-3.6406e-08,  1.1464e-08],\n",
      "        [-3.6406e-08, -1.1464e-08],\n",
      "        [ 3.8060e-08,  0.0000e+00],\n",
      "        [-2.5385e-08,  9.9109e-09],\n",
      "        [-2.5385e-08, -9.9109e-09],\n",
      "        [ 1.7148e-08,  1.6002e-08],\n",
      "        [ 1.7148e-08, -1.6002e-08],\n",
      "        [ 2.1627e-08,  4.5454e-09],\n",
      "        [ 2.1627e-08, -4.5454e-09],\n",
      "        [-5.6450e-09,  2.0894e-08],\n",
      "        [-5.6450e-09, -2.0894e-08],\n",
      "        [-2.0406e-08,  7.4463e-09],\n",
      "        [-2.0406e-08, -7.4463e-09],\n",
      "        [-1.3149e-08,  1.5559e-08],\n",
      "        [-1.3149e-08, -1.5559e-08],\n",
      "        [ 5.3209e-09,  1.7347e-08],\n",
      "        [ 5.3209e-09, -1.7347e-08],\n",
      "        [ 1.8653e-08,  0.0000e+00],\n",
      "        [ 1.1276e-08,  1.4025e-08],\n",
      "        [ 1.1276e-08, -1.4025e-08],\n",
      "        [ 1.3356e-08,  9.2881e-09],\n",
      "        [ 1.3356e-08, -9.2881e-09],\n",
      "        [-9.8043e-09,  1.1056e-08],\n",
      "        [-9.8043e-09, -1.1056e-08],\n",
      "        [-1.1595e-08,  6.8438e-09],\n",
      "        [-1.1595e-08, -6.8438e-09],\n",
      "        [-1.1595e-08,  0.0000e+00],\n",
      "        [-5.8601e-09,  9.2283e-09],\n",
      "        [-5.8601e-09, -9.2283e-09],\n",
      "        [ 1.1473e-08,  3.4028e-09],\n",
      "        [ 1.1473e-08, -3.4028e-09],\n",
      "        [ 9.7098e-10,  9.6045e-09],\n",
      "        [ 9.7098e-10, -9.6045e-09],\n",
      "        [ 8.4601e-09,  5.2821e-09],\n",
      "        [ 8.4601e-09, -5.2821e-09],\n",
      "        [-1.1281e-09,  7.7821e-09],\n",
      "        [-1.1281e-09, -7.7821e-09],\n",
      "        [-1.9591e-09,  6.6352e-09],\n",
      "        [-1.9591e-09, -6.6352e-09],\n",
      "        [-7.8555e-09,  0.0000e+00],\n",
      "        [-6.9776e-09,  2.3876e-09],\n",
      "        [-6.9776e-09, -2.3876e-09],\n",
      "        [ 7.0232e-09,  2.0119e-09],\n",
      "        [ 7.0232e-09, -2.0119e-09],\n",
      "        [ 6.3703e-09,  0.0000e+00],\n",
      "        [-3.7790e-09,  4.7246e-09],\n",
      "        [-3.7790e-09, -4.7246e-09],\n",
      "        [-5.1366e-09,  8.9675e-10],\n",
      "        [-5.1366e-09, -8.9675e-10],\n",
      "        [ 2.3853e-09,  1.8745e-09],\n",
      "        [ 2.3853e-09, -1.8745e-09],\n",
      "        [ 3.7099e-09,  0.0000e+00],\n",
      "        [-1.8221e-09,  0.0000e+00],\n",
      "        [-8.1422e-10,  0.0000e+00],\n",
      "        [ 6.4574e-14,  0.0000e+00]], grad_fn=<EigBackward>)\n",
      "tensor([[-0.0621, -0.0621, -0.0618,  ...,  0.0069,  0.0061,  0.0061],\n",
      "        [-0.0621, -0.0621, -0.0618,  ...,  0.0069,  0.0061,  0.0061],\n",
      "        [-0.0618, -0.0618, -0.0621,  ...,  0.0078,  0.0069,  0.0069],\n",
      "        ...,\n",
      "        [ 0.0069,  0.0069,  0.0078,  ..., -0.0621, -0.0618, -0.0618],\n",
      "        [ 0.0061,  0.0061,  0.0069,  ..., -0.0618, -0.0621, -0.0621],\n",
      "        [ 0.0061,  0.0061,  0.0069,  ..., -0.0618, -0.0621, -0.0621]],\n",
      "       grad_fn=<CatBackward>)\n"
     ]
    },
    {
     "ename": "NotPSDError",
     "evalue": "Matrix not positive definite after repeatedly adding jitter up to 1.0e-04. Original error on first attempt: cholesky_cpu: U(8,8) is zero, singular U.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/container_storage/sage/DiffEqGPs/gpytorch/utils/cholesky.py\u001b[0m in \u001b[0;36m_psd_safe_cholesky\u001b[0;34m(A, out, jitter, max_tries)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cholesky_cpu: U(8,8) is zero, singular U.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotPSDError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3cb9a87abac5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Calc loss and backprop gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mmll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m#print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f  variance: %.3f noise: %.3f' % (\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/container_storage/sage/DiffEqGPs/gpytorch/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_validate_module_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/container_storage/sage/DiffEqGPs/gpytorch/mlls/exact_marginal_log_likelihood.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, function_dist, target, *params)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Get the log prob of the marginal distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_other_terms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/container_storage/sage/DiffEqGPs/gpytorch/distributions/multitask_multivariate_normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mnew_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/container_storage/sage/DiffEqGPs/gpytorch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;31m# Get log determininant and first part of quadratic form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mcovar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcovar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0minv_quad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcovar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_quad_logdet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_quad_rhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minv_quad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/container_storage/sage/DiffEqGPs/gpytorch/lazy/lazy_tensor.py\u001b[0m in \u001b[0;36minv_quad_logdet\u001b[0;34m(self, inv_quad_rhs, logdet, reduce_inv_quad)\u001b[0m\n\u001b[1;32m   1238\u001b[0m                     \u001b[0mwill_need_cholesky\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwill_need_cholesky\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m                 \u001b[0mcholesky\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCholLazyTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTriangularLazyTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcholesky\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_quad_logdet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_quad_rhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minv_quad_rhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogdet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_inv_quad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce_inv_quad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/container_storage/sage/DiffEqGPs/gpytorch/lazy/lazy_tensor.py\u001b[0m in \u001b[0;36mcholesky\u001b[0;34m(self, upper)\u001b[0m\n\u001b[1;32m    957\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mLazyTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mCholesky\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtriangular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlower\u001b[0m \u001b[0mdepending\u001b[0m \u001b[0mon\u001b[0m \u001b[0;34m\"upper\"\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \"\"\"\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0mchol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0mchol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transpose_nonbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/container_storage/sage/DiffEqGPs/gpytorch/utils/memoize.py\u001b[0m in \u001b[0;36mg\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mkwargs_pkl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_in_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_add_to_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_get_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/container_storage/sage/DiffEqGPs/gpytorch/lazy/lazy_tensor.py\u001b[0m in \u001b[0;36m_cholesky\u001b[0;34m(self, upper)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# contiguous call is necessary here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mcholesky\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsd_safe_cholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluated_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTriangularLazyTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/container_storage/sage/DiffEqGPs/gpytorch/utils/cholesky.py\u001b[0m in \u001b[0;36mpsd_safe_cholesky\u001b[0;34m(A, upper, out, jitter, max_tries)\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mNumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mattempts\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mwith\u001b[0m \u001b[0msuccessively\u001b[0m \u001b[0mincreasing\u001b[0m \u001b[0mjitter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mto\u001b[0m \u001b[0mmake\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0mraising\u001b[0m \u001b[0man\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \"\"\"\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_psd_safe_cholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjitter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjitter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_tries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/container_storage/sage/DiffEqGPs/gpytorch/utils/cholesky.py\u001b[0m in \u001b[0;36m_psd_safe_cholesky\u001b[0;34m(A, out, jitter, max_tries)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             raise NotPSDError(\n\u001b[0m\u001b[1;32m     88\u001b[0m                 \u001b[0;34mf\"Matrix not positive definite after repeatedly adding jitter up to {jitter_new:.1e}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0;34mf\"Original error on first attempt: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotPSDError\u001b[0m: Matrix not positive definite after repeatedly adding jitter up to 1.0e-04. Original error on first attempt: cholesky_cpu: U(8,8) is zero, singular U."
     ]
    }
   ],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iter = int(2) if smoke_test else int(20)\n",
    "\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=float(0.1))  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    for parameter in model.named_parameters():\n",
    "        print(parameter)\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    #print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f  variance: %.3f noise: %.3f' % (\n",
    "    #    i + 1, training_iter, loss.item(),\n",
    "    #    model.covar_module.length.item(),\n",
    "    #    model.covar_module.var.item(),\n",
    "    #    model.likelihood.noise.item()\n",
    "    #))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "75730997",
   "metadata": {},
   "source": [
    "model.named_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d1b8824",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('likelihood.raw_task_noises', Parameter containing:\n",
      "tensor([0., 0.], requires_grad=True))\n",
      "('likelihood.raw_noise', Parameter containing:\n",
      "tensor([0.], requires_grad=True))\n",
      "('covar_module.12015766640.var', Parameter containing:\n",
      "tensor(1., requires_grad=True))\n",
      "('covar_module.12015766640.length', Parameter containing:\n",
      "tensor(1., requires_grad=True))\n",
      "('covar_module.12015765968.var', Parameter containing:\n",
      "tensor(1., requires_grad=True))\n",
      "('covar_module.12015765968.length', Parameter containing:\n",
      "tensor(1., requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for parameter in model.named_parameters():\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd409d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c869eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set into eval mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Initialize plots\n",
    "\n",
    "number_of_samples = int(50)\n",
    "# Make predictions\n",
    "with torch.no_grad():#, gpytorch.settings.fast_pred_var():\n",
    "    test_x = torch.linspace(float(0), float(2), number_of_samples)\n",
    "    #pdb.set_trace()\n",
    "    outputs = model(test_x)\n",
    "    predictions = likelihood(outputs)\n",
    "    \n",
    "    mean = predictions.mean\n",
    "    lower, upper = predictions.confidence_region()\n",
    "#print(mean)\n",
    "#print(lower)\n",
    "#print(upper)\n",
    "# This contains predictions for both tasks, flattened out\n",
    "# The first half of the predictions is for the first task\n",
    "# The second half is for the second task\n",
    "\n",
    "#dims = int(2)\n",
    "#indices = [list(range(i, len(train_y), dims)) for i in range(dims)]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49b79859",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4df72d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a03a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (y1_ax, y2_ax) = plt.subplots(int(1), int(2), figsize=(int(8), int(3)))\n",
    "\n",
    "# Plot training data as black stars\n",
    "y1_ax.plot(train_x.detach().numpy(), train_y[:, 0].detach().numpy(), 'k*')\n",
    "# Predictive mean as blue line\n",
    "y1_ax.plot(test_x.numpy(), mean[:, 0].numpy(), 'b')\n",
    "# Shade in confidence\n",
    "y1_ax.fill_between(test_x.numpy(), lower[:, 0].numpy(), upper[:, 0].numpy(), alpha=0.5)\n",
    "y1_ax.set_ylim([-3, 8])\n",
    "y1_ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "y1_ax.set_title('Observed Values (Likelihood)')\n",
    "\n",
    "# Plot training data as black stars\n",
    "y2_ax.plot(train_x.detach().numpy(), train_y[:, 1].detach().numpy(), 'k*')\n",
    "# Predictive mean as blue line\n",
    "y2_ax.plot(test_x.numpy(), mean[:, 1].numpy(), 'b')\n",
    "# Shade in confidence\n",
    "y2_ax.fill_between(test_x.numpy(), lower[:, 1].numpy(), upper[:, 1].numpy(), alpha=0.5)\n",
    "y2_ax.set_ylim([-3, 8])\n",
    "y2_ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "y2_ax.set_title('Observed Values (Likelihood)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822f0426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf73a6c3",
   "metadata": {},
   "source": [
    "# Test Diffable SE Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b432934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([int(1), int(2), int(3)])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae01ece4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46856bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1, x2, l, sigma = var('x1, x2, l, sigma')\n",
    "lengthscale = 1\n",
    "variance = 1\n",
    "SE(x1, x2, l, sigma) = sigma^2*exp(-(x1-x2)^2/(2*l^2))\n",
    "cov_matr = [[None for i in range(len(X))] for j in range(len(X))]\n",
    "for i, (v1, v2) in enumerate(product(X, X)):\n",
    "    cov_matr[int(i/len(X))][int(i%len(X))] = float(SE.diff(x2).diff(x1)(int(v1), int(v2), lengthscale, variance))\n",
    "cov_matr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bee06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SE.operands()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4620c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Diff_SE_kernel(var=int(variance), length=int(lengthscale))\n",
    "q, dx1, dx2 = var('q, dx1, dx2')\n",
    "left_poly = dx2\n",
    "right_poly = dx1 \n",
    "diffed_kernel = a.diff(left_poly=left_poly, right_poly=right_poly, left_d_var=var('dx2'), right_d_var=var('dx1'))\n",
    "left_poly = dx2\n",
    "right_poly = 1\n",
    "diffed_kernel2 = a.diff(left_poly=left_poly, right_poly=right_poly, left_d_var=var('dx2'), right_d_var=var('dx1'))\n",
    "diffed_kernel(X).evaluate() + diffed_kernel2(X).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e3474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cell_diff(L, M, R, context=None):\n",
    "    len_M = np.shape(M)[0]\n",
    "    temp = None\n",
    "    # https://stackoverflow.com/questions/6473679/transpose-list-\n",
    "    # of-lists\n",
    "    M_transpose = list(\n",
    "       map(list, itertools.zip_longest(*M, fillvalue=None)))\n",
    "    for r_elem, row_M in zip(R, M_transpose):\n",
    "        for l_elem, m_elem in zip(L, row_M):\n",
    "            if temp is None:\n",
    "                #if M_transpose[int(j/len_M)][j % len_M] is not None:\n",
    "                if m_elem is not None:\n",
    "                    temp = l_elem * m_elem*r_elem\n",
    "                    #temp = l_elem * M_transpose[int(j/len_M)][j % len_M]*r_elem\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                if m_elem is not None:\n",
    "                #if M_transpose[int(j/len_M)][j % len_M] is not None:\n",
    "                    temp += l_elem * m_elem*r_elem\n",
    "                    #temp += l_elem * M_transpose[int(j/len_M)][j % len_M]*r_elem\n",
    "                else:\n",
    "                    pass\n",
    "    return temp.simplify_full()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a14736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 3\n",
    "length = dimension*dimension +1\n",
    "L_list = [var(f'l_{i}{j}') for i in range(1, dimension+1) for j in range(1, dimension+1)]\n",
    "M_list = [var(f'm_{i}{j}') for i in range(1, dimension+1) for j in range(1, dimension+1)]\n",
    "R_list = [var(f'r_{i}{j}') for i in range(1, dimension+1) for j in range(1, dimension+1)]\n",
    "L = matrix(dimension, dimension, L_list)\n",
    "M = matrix(dimension, dimension, M_list)\n",
    "R = matrix(dimension, dimension, R_list)\n",
    "print(L)\n",
    "print(M)\n",
    "print(R)\n",
    "row = 0\n",
    "col = 0\n",
    "for row in range(dimension):\n",
    "    for col in range(dimension):\n",
    "        print((L*M*R)[row][col])\n",
    "print(\"\\n\\n\")\n",
    "for i, (l, r) in enumerate(itertools.product(L.rows(), R.columns())):\n",
    "\n",
    "    print(calc_cell_diff(l, M, r))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5347513f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb35080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbb4445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cell_diff_sage(L, M, R, context=None):\n",
    "    temp = None\n",
    "    # https://stackoverflow.com/questions/6473679/transpose-list-\n",
    "    # of-lists\n",
    "    M_transpose = list(\n",
    "        map(list, itertools.zip_longest(*M, fillvalue=None)))\n",
    "    # Every row in 'M' is combined with each elem of the row given in 'R'\n",
    "    # Or: For each elemtn in row 'R' combine with 'row_M'\n",
    "    for r_elem, row_M in zip(R, M_transpose):\n",
    "        # Each element in L gets exactly one element in 'row_M' to multiply\n",
    "        # Or: Combine each element in row_M with exactly one element in 'L'\n",
    "        for l_elem, m_elem in zip(L, row_M):\n",
    "            if temp is None:\n",
    "                if m_elem is not None:\n",
    "                    if not l_elem == 0 and not r_elem == 0:\n",
    "                        temp = m_elem.diff(l_elem).diff(r_elem)\n",
    "                    #elif l_elem == 0 and not r_elem == 0:\n",
    "                    #    temp = m_elem.diff(r_elem)\n",
    "                    #elif not l_elem == 0 and r_elem == 0:\n",
    "                    #    temp = m_elem.diff(l_elem)\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                if m_elem is not None:\n",
    "                    if not l_elem == 0 and not r_elem == 0:\n",
    "                        temp += m_elem.diff(l_elem).diff(r_elem)\n",
    "                    #elif l_elem == 0 and not r_elem == 0:\n",
    "                    #    temp += m_elem.diff(r_elem)\n",
    "                    #elif not l_elem == 0 and r_elem == 0:\n",
    "                    #    temp += m_elem.diff(l_elem)\n",
    "                    \n",
    "                else:\n",
    "                    pass\n",
    "    return temp\n",
    "\n",
    "def diff_sage(matrix, left_matrix=None, right_matrix=None):\n",
    "    # iterate left matrix by rows and right matrix by columns and call the\n",
    "    # respective diff command of the kernels with the row/cols as params\n",
    "    kernel = MatrixKernel(None)\n",
    "    output_matrix = [[0 for i in range(np.shape(matrix)[1])] for j in range(np.shape(matrix)[0])]\n",
    "    for i, (l, r) in enumerate(itertools.product(left_matrix.rows(), right_matrix.columns())):\n",
    "        res = calc_cell_diff_sage(l, matrix, r, context=kernel)\n",
    "        output_matrix[int(i/np.shape(matrix)[0])][\n",
    "                    int(i % np.shape(matrix)[0])]  = res\n",
    "    kernel.set_matrix(output_matrix)\n",
    "    return output_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f7f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = matrix(2, 2, (x1, x1, 0, x1))\n",
    "R = matrix(2, 2, (x2, 0, x2, x2))\n",
    "x1, x2, l, sigma = var('x1, x2, l, sigma')\n",
    "lengthscale = 1\n",
    "variance = 1\n",
    "SEKernelMatrix = [[sigma^2*exp(-(x1-x2)^2/(2*l^2)), None], [None, sigma^2*exp(-(x1-x2)^2/(2*l^2))]]\n",
    "diffed_SE_sage_matrix_kernel = diff_sage(SEKernelMatrix, left_matrix=L, right_matrix=R)\n",
    "pprint.pprint(diffed_SE_sage_matrix_kernel)\n",
    "cov_matr = [[None for i in range(len(X)*len(diffed_SE_sage_matrix_kernel))] for j in range(len(X)*len(diffed_SE_sage_matrix_kernel))]\n",
    "for i, (v1, v2) in enumerate(product(X, X)):\n",
    "    for row in range(len(diffed_SE_sage_matrix_kernel)):\n",
    "        for col in range(len(diffed_SE_sage_matrix_kernel)):\n",
    "            # Blockwise\n",
    "            cov_matr[int(i/len(X))+row*len(X)][int(i%len(X))+col*len(X)] = diffed_SE_sage_matrix_kernel[row][col].substitute(x1=int(v1), x2=int(v2), l=lengthscale, sigma=variance)\n",
    "            # Interleaved\n",
    "            #cov_matr[int(((i*len(diffed_SE_sage_matrix_kernel))+row)/(len(X)*len(diffed_SE_sage_matrix_kernel)))*2+row][int((i*len(diffed_SE_sage_matrix_kernel))+col)%(len(X)*len(diffed_SE_sage_matrix_kernel))] = float(diffed_SE_sage_matrix_kernel[row][col].substitute(x1=int(v1), x2=int(v2), l=lengthscale, sigma=variance))\n",
    "cov_matr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548fa203",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kernel = Diff_SE_kernel()\n",
    "kernel2 = Diff_SE_kernel()\n",
    "q, dx1, dx2 = var('q, dx1, dx2')\n",
    "L = matrix(2, 2, (dx1, dx1, 0, dx1))\n",
    "R = matrix(2, 2, (dx2, 0, dx2, dx2))\n",
    "\n",
    "p = DiffMatrixKernel([[kernel, None], [None, kernel2]])\n",
    "covar_module = p.diff(left_matrix=L, right_matrix=R)\n",
    "\n",
    "covar_x = covar_module(X)\n",
    "covar_x.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c54aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "matr = [[2, 0, -6*e^(-2), 1, e^(-1/2), -e^(-2)],\n",
    " [0, 2, 0, -e^(-1/2), 1, e^(-1/2)],\n",
    " [-6*e^(-2), 0, 2, -5*e^(-2), -e^(-1/2), 1],\n",
    " [1, e^(-1/2), -e^(-2), 1, 0, -3*e^(-2)],\n",
    " [-e^(-1/2), 1, e^(-1/2), 0, 1, 0],\n",
    " [-5*e^(-2), -e^(-1/2), 1, -3*e^(-2), 0, 1]]\n",
    "\n",
    "matr = [[2, 0, -6*e^(-2), 1, 0, -3*e^(-2)],\n",
    " [0, 2, 0, 0, 1, 0],\n",
    " [-6*e^(-2), 0, 2, -3*e^(-2), 0, 1],\n",
    " [1, 0, -3*e^(-2), 1, 0, -3*e^(-2)],\n",
    " [0, 1, 0, 0, 1, 0],\n",
    " [-3*e^(-2), 0, 1, -3*e^(-2), 0, 1]]\n",
    "\n",
    "matr = torch.Tensor(matr)\n",
    "import pprint\n",
    "pprint.pprint(matr)\n",
    "print(matr[0::3, 0::3])\n",
    "H_x = 3\n",
    "torch.vstack([torch.hstack([matr[k::H_x, l::H_x] for l in range(H_x)]) for k in range(H_x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4827e3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b970f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class testobject():\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "    \n",
    "    def setVal(self, val):\n",
    "        self.val = val\n",
    "        \n",
    "    def printVal(self):\n",
    "        return self.val\n",
    "    \n",
    "    def __call__(self):\n",
    "        return self.val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d23c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = testobject(42)\n",
    "t2 = testobject(21)\n",
    "t3 = testobject(17)\n",
    "l = [[t1, t2], [t2, t3]]\n",
    "print(l)\n",
    "t2.setVal(170)\n",
    "print(l[0][1].printVal())\n",
    "print(l[1][0].printVal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f894c2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900df7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "q, dx1, dx2 = var('q, dx1, dx2')\n",
    "left_poly = dx1\n",
    "right_poly = dx2\n",
    "L = matrix(2, 2, (dx1, 0, 0, dx1))\n",
    "R = matrix(2, 2, (dx2, 0, 0, dx2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234faf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.diff(left_matrix=L, right_matrix=R).forward(X, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a46303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce7622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, q, dx1, dx2 = var('w, q, dx1, dx2')\n",
    "a = dx1^2\n",
    "#a.degree(dx1)\n",
    "a.operands()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a98d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d88618",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([[int(1), int(2), int(3)], [int(4), int(5), int(6)], [int(7), int(8), int(9)]])\n",
    "for i, row in enumerate(a):\n",
    "    for j, elem in enumerate(row[i:]):\n",
    "        print(f\"row: {i}, col: {i+j}\")\n",
    "        print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22521611",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c, d = var('a, b, c, d')\n",
    "A = matrix(2,2, (a, b, c, d))\n",
    "B = matrix(2, 2, (dx1, dx1, 0, dx1))\n",
    "C = matrix(2, 2, (dx2, 0, dx2, dx2))\n",
    "print(A)\n",
    "print(B)\n",
    "B*A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd9423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cdd599",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['a', 'b', 'c']\n",
    "y = x                 # x and y reference the same object\n",
    "z = ['a', 'b', 'c']   # x and z reference different objects\n",
    "#z\n",
    "\n",
    "\n",
    "print(x is z)\n",
    "z = x\n",
    "print(x is z)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 9.2",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
